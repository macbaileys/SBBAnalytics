{
    "nbformat": 4,
    "nbformat_minor": 4,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Swiss Train Station Analysis**\n",
                "\n",
                "This notebook demonstrates a comprehensive analysis of Swiss train station data, which contains:\n",
                "\n",
                "- **Bahnhof**: Station name\n",
                "- **Kanton**: Swiss canton/province\n",
                "- **DTV**: Average daily traffic\n",
                "- **DWV**: Average workday traffic (Mon-Fri)\n",
                "- **DNWV**: Average non-workday traffic (Sat, Sun, Holidays)\n",
                "- **EVU**: Train operator\n",
                "- **lon, lat**: Coordinates\n",
                "\n",
                "We will walk through:\n",
                "1. **Data Loading & Cleaning**\n",
                "2. **Descriptive Statistics & Visualizations** (including how to handle skewed data)\n",
                "3. **Chi-Squared Test** (Kanton vs. EVU)\n",
                "4. **One-Way ANOVA** (DTV by Kanton)\n",
                "5. **Correlation Analysis** (DTV, DWV, DNWV)\n",
                "\n",
                "We also demonstrate how to handle **outliers** and **log-scale** transformations, given the highly skewed nature of traffic data.\n",
                "\n",
                "## **Topic**: *Differences in Daily Traffic (DTV) Across Cantons*\n",
                "We'll see if certain cantons have significantly different average daily traffic, and whether traffic is related to train operator.\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "collapsed": false
            },
            "execution_count": null,
            "outputs": [],
            "source": [
                "# Standard library imports\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "from scipy.stats import chi2_contingency, f_oneway, pearsonr, levene, shapiro\n",
                "\n",
                "# Visualization imports\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Optional: for Jupyter inline plotting\n",
                "%matplotlib inline\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Cleaning\n",
                "\n",
                "Replace `'stations.csv'` below with your actual CSV file if needed.\n",
                "We assume your CSV has columns:\n",
                "```\n",
                "Code,UIC,Bahnhof,Kanton,ISB_GI,Jahr,DTV,DWV,DNWV,EVU,lon,lat\n",
                "```\n",
                "and ~3,471 rows. Here we load the data into a pandas DataFrame.\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "# Step 1: Load CSV\n",
                "csv_file = 'stations.csv'  # <-- Change if needed\n",
                "df = pd.read_csv(csv_file)\n",
                "\n",
                "print(\"DataFrame Shape:\", df.shape)\n",
                "df.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.1 Check for Missing or Invalid Data\n",
                "We check whether the columns we need are present, and how many missing values exist. If necessary, we can drop or impute them."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "# Checking for missing values\n",
                "df.isna().sum()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If needed, we can **drop** rows with critical missing columns (e.g., `DTV`, `DWV`, `DNWV`, `Kanton`, `EVU`) or **impute** them. Below we demonstrate dropping them for clarity."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "# We define a list of required columns for analysis\n",
                "required_cols = ['Kanton','EVU','DTV','DWV','DNWV']\n",
                "df.dropna(subset=required_cols, inplace=True)\n",
                "\n",
                "# Convert numeric columns if they're strings\n",
                "for col in ['DTV','DWV','DNWV','lon','lat']:\n",
                "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
                "\n",
                "# Drop rows that still have NaN in these columns after conversion\n",
                "df.dropna(subset=['DTV','DWV','DNWV','lon','lat'], inplace=True)\n",
                "\n",
                "print(\"Final shape after cleaning:\", df.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Descriptive Statistics & Visualizations\n",
                "Let's get a sense of the numeric columns, especially how skewed `DTV` might be."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "# Basic descriptive stats for DTV, DWV, DNWV\n",
                "df[['DTV','DWV','DNWV']].describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Distribution of DTV\n",
                "Traffic data can be **highly skewed**, with many small/medium stations and a few extremely large ones. Let's visualize it."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8,5))\n",
                "sns.histplot(data=df, x='DTV', kde=True, color='blue')\n",
                "plt.title('Distribution of DTV (Linear Scale)')\n",
                "plt.xlabel('DTV')\n",
                "plt.ylabel('Count')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Because the data are often **heavily skewed**, you might see a single spike near 0 and a very long tail. Let's also plot it on a **log scale** for better visibility."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8,5))\n",
                "sns.histplot(data=df, x='DTV', kde=True, color='blue')\n",
                "plt.xscale('log')\n",
                "plt.title('Distribution of DTV (Log Scale)')\n",
                "plt.xlabel('DTV (log scale)')\n",
                "plt.ylabel('Count')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Descriptive Insights by Canton\n",
                "Let's look at how many station entries there are per `Kanton`."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "kanton_counts = df['Kanton'].value_counts()\n",
                "kanton_counts"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Visualize counts per canton with a bar plot."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10,5))\n",
                "sns.barplot(x=kanton_counts.index, y=kanton_counts.values, color='green')\n",
                "plt.title('Number of Station Rows per Kanton')\n",
                "plt.xlabel('Kanton')\n",
                "plt.ylabel('Count')\n",
                "plt.xticks(rotation=45)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Chi-Squared Test (Kanton vs. EVU)\n",
                "\n",
                "We test the **association** between two categorical variables:\n",
                "- `Kanton` (canton)\n",
                "- `EVU` (train operator)\n",
                "\n",
                "**Null Hypothesis (H0)**: There is *no* association between Kanton and EVU.\n",
                "\n",
                "**Alternative (H1)**: There *is* some association (the distribution of EVU depends on Kanton or vice-versa)."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "contingency_table = pd.crosstab(df['Kanton'], df['EVU'])\n",
                "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
                "\n",
                "print(\"--- Chi-Squared Test: Kanton vs EVU ---\")\n",
                "print(\"Chi2 Statistic:\", chi2_stat)\n",
                "print(\"p-value:\", p_value)\n",
                "print(\"Degrees of Freedom:\", dof)\n",
                "print(\"Expected Frequency Table:\")\n",
                "print(expected)\n",
                "\n",
                "if p_value < 0.05:\n",
                "    print(\"\\nConclusion: p-value < 0.05 => There is a statistically significant association between Kanton and EVU.\")\n",
                "else:\n",
                "    print(\"\\nConclusion: p-value >= 0.05 => No strong evidence of association between Kanton and EVU.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. One-Way ANOVA (DTV by Kanton)\n",
                "\n",
                "We want to see if the **mean DTV** differs significantly **across cantons**.\n",
                "\n",
                "**Null Hypothesis (H0)**: All cantons have the *same* mean DTV.\n",
                "\n",
                "**Alternative (H1)**: At least one canton has a different mean DTV.\n",
                "\n",
                "### 4.1 Check ANOVA Assumptions\n",
                "- **Normality** in each group (somewhat robust if sample sizes are large)\n",
                "- **Homogeneity of variances** across groups\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "# Group the data by Kanton\n",
                "grouped = df.groupby('Kanton')['DTV']\n",
                "groups_list = [grouped.get_group(k) for k in grouped.groups]\n",
                "\n",
                "# Levene's Test for equal variances\n",
                "levene_stat, levene_p = levene(*groups_list)\n",
                "print(\"--- Levene's Test for Homogeneity of Variances (DTV by Kanton) ---\")\n",
                "print(\"Levene Statistic:\", levene_stat)\n",
                "print(\"p-value:\", levene_p)\n",
                "if levene_p < 0.05:\n",
                "    print(\"\\nConclusion: p-value < 0.05 => Variances may not be equal.\")\n",
                "else:\n",
                "    print(\"\\nConclusion: p-value >= 0.05 => Variances are likely equal.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Perform ANOVA\n",
                "We use `scipy.stats.f_oneway` for a one-way ANOVA."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "f_stat, p_val = f_oneway(*groups_list)\n",
                "print(\"\\n--- One-Way ANOVA: DTV by Kanton ---\")\n",
                "print(\"F-statistic:\", f_stat)\n",
                "print(\"p-value:\", p_val)\n",
                "\n",
                "if p_val < 0.05:\n",
                "    print(\"\\nConclusion: p-value < 0.05 => At least one canton has a different mean DTV.\")\n",
                "else:\n",
                "    print(\"\\nConclusion: p-value >= 0.05 => No significant difference in mean DTV among cantons.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 4.3 (Optional) Post-Hoc Test\n",
                "If ANOVA is significant, you might want to know *which* cantons differ. You can use tools like **Tukey's HSD** from `statsmodels.sandbox.stats.multicomp`. \n",
                "This is optional, but recommended if you get a significant result and want to see pairwise differences."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "# Example (commented out by default):\n",
                "# !pip install statsmodels  # If not installed\n",
                "try:\n",
                "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
                "    # Prepare data for Tukey\n",
                "    dtv_data = df[['Kanton','DTV']].dropna()\n",
                "    tukey = pairwise_tukeyhsd(endog=dtv_data['DTV'], groups=dtv_data['Kanton'], alpha=0.05)\n",
                "    print(tukey)\n",
                "except ImportError:\n",
                "    print(\"Install statsmodels to run Tukey's HSD post-hoc test.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Correlation Analyses\n",
                "### 5.1 Correlation between DTV and DWV\n",
                "We hypothesize DTV (average daily traffic) and DWV (average workday traffic) might be strongly correlated."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "corr_coeff, corr_pval = pearsonr(df['DTV'], df['DWV'])\n",
                "print(\"--- Pearson Correlation: DTV vs DWV ---\")\n",
                "print(\"Correlation Coefficient =\", corr_coeff)\n",
                "print(\"p-value =\", corr_pval)\n",
                "\n",
                "if corr_pval < 0.05:\n",
                "    print(\"Conclusion: p-value < 0.05 => Significant correlation.\")\n",
                "else:\n",
                "    print(\"Conclusion: p-value >= 0.05 => Not significant.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Correlation between DTV and DNWV"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "corr_coeff_dnwv, corr_pval_dnwv = pearsonr(df['DTV'], df['DNWV'])\n",
                "print(\"\\n--- Pearson Correlation: DTV vs DNWV ---\")\n",
                "print(\"Correlation Coefficient =\", corr_coeff_dnwv)\n",
                "print(\"p-value =\", corr_pval_dnwv)\n",
                "\n",
                "if corr_pval_dnwv < 0.05:\n",
                "    print(\"Conclusion: p-value < 0.05 => Significant correlation.\")\n",
                "else:\n",
                "    print(\"Conclusion: p-value >= 0.05 => Not significant.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 Correlation between DWV and DNWV"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "corr_coeff_wv_nwv, corr_pval_wv_nwv = pearsonr(df['DWV'], df['DNWV'])\n",
                "print(\"\\n--- Pearson Correlation: DWV vs DNWV ---\")\n",
                "print(\"Correlation Coefficient =\", corr_coeff_wv_nwv)\n",
                "print(\"p-value =\", corr_pval_wv_nwv)\n",
                "\n",
                "if corr_pval_wv_nwv < 0.05:\n",
                "    print(\"Conclusion: p-value < 0.05 => Significant correlation.\")\n",
                "else:\n",
                "    print(\"Conclusion: p-value >= 0.05 => Not significant.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.4 Correlation Matrix & Heatmap\n",
                "A quick overview of correlations among `DTV`, `DWV`, and `DNWV`."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "num_cols = ['DTV','DWV','DNWV']\n",
                "corr_matrix = df[num_cols].corr(method='pearson')\n",
                "print(\"\\nCorrelation Matrix:\")\n",
                "print(corr_matrix)\n",
                "\n",
                "plt.figure(figsize=(5,4))\n",
                "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
                "plt.title('Correlation Heatmap: DTV, DWV, DNWV')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Additional Insights / Handling Outliers\n",
                "\n",
                "Given the extreme skew in `DTV`, you might:\n",
                "1. Use **log-scale** in your plots.\n",
                "2. **Cap** or **clip** outliers above a certain percentile (e.g., 99th).\n",
                "3. Investigate large stations individually.\n",
                "\n",
                "Below is an example of **clipping** at the 99th percentile, then re-plotting the distribution to see the core data more clearly."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "pct_99 = np.percentile(df['DTV'], 99)\n",
                "df_clipped = df[df['DTV'] <= pct_99]\n",
                "\n",
                "plt.figure(figsize=(8,5))\n",
                "sns.histplot(data=df_clipped, x='DTV', kde=True, color='red')\n",
                "plt.title('Distribution of DTV (Clipped at 99th Percentile)')\n",
                "plt.xlabel('DTV (clipped)')\n",
                "plt.ylabel('Count')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Clipped at 99th percentile of DTV: {pct_99:.2f}\")\n",
                "print(\"Shape before clipping:\", df.shape, \"Shape after clipping:\", df_clipped.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary of Findings & Next Steps\n",
                "\n",
                "1. **Data Cleaning**:\n",
                "   - We removed rows missing critical columns.\n",
                "   - Confirmed heavy skew in DTV.\n",
                "2. **Descriptive Statistics**:\n",
                "   - Many stations have lower traffic, few have extremely high traffic.\n",
                "   - Distribution is highly skewed.\n",
                "3. **Chi-Squared**:\n",
                "   - We tested whether there's an association between `Kanton` and `EVU`.\n",
                "   - If p-value < 0.05, there's a significant association.\n",
                "4. **ANOVA** (DTV by Kanton):\n",
                "   - We tested if at least one canton differs in mean DTV.\n",
                "   - If p-value < 0.05, investigate further (e.g., Tukeyâ€™s HSD).\n",
                "5. **Correlation**:\n",
                "   - We checked how `DTV` relates to `DWV` and `DNWV`.\n",
                "\n",
                "### Possible Next Steps\n",
                "- **Filter by Year** (e.g., compare 2022 vs. 2023).\n",
                "- **Investigate EVU** differences in traffic.\n",
                "- **Geospatial Analysis**: Plot stations on a map (with `lon` & `lat`) colored by traffic volume. Libraries like `folium` or `geopandas` are handy.\n",
                "- **Regression Models**: Predict DTV based on location, operator, or other features.\n",
                "\n",
                "## End of Notebook\n",
                "We hope this provides a **great in-depth** starting point for your Swiss train station dataset analysis!"
            ]
        }
    ]
}